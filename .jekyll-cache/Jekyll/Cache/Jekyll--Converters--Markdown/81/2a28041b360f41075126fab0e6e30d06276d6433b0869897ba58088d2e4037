I"i-<p>This page provides recommended <code class="language-plaintext highlighter-rouge">Executioner</code> and <code class="language-plaintext highlighter-rouge">Preconditioning</code>
settings for new Moltres users who already have basic proficiency with MOOSE. The
following sections provide recommendations on the solve type, automatic scaling, time
integration scheme, and preconditioning settings.</p>

<p>Moltres, like all MOOSE-based apps, rely on PETSc for preconditioning. Refer to MOOSE’s
documentation
<a href="https://mooseframework.inl.gov/syntax/Preconditioning/index.html">here</a> for
basic information on preconditioning in MOOSE. You may also refer to the example input file
<a href="http://arfc.github.io/software/moltres/wiki/input_example/">here</a> for an introduction
to the Moltres input file format.</p>

<h2 id="solve-type">Solve Type</h2>

<p>Previous experiences have shown that Moltres simulations usually require the <code class="language-plaintext highlighter-rouge">NEWTON</code>
solve type with all off-diagonal Jacobian entries enabled because reactor neutronics
problems are very non-linear from the strong coupling between the neutron group flux
and delayed neutron precursor concentration variables. Users may use these settings by
setting <code class="language-plaintext highlighter-rouge">solve_type = NEWTON</code> and <code class="language-plaintext highlighter-rouge">full = true</code> in the <code class="language-plaintext highlighter-rouge">Executioner</code> and
<code class="language-plaintext highlighter-rouge">Preconditioning</code> blocks, respectively, in the input file.</p>

<h2 id="automatic-scaling">Automatic Scaling</h2>

<p>Relevant variables in a Moltres simulation include neutron group fluxes, delayed neutron
precursor concentrations, temperature, and velocity components. These variables differ
significantly in magnitude, especially between the neutronics and thermal-hydraulics
variables. We recommend applying scaling factors to the variables so that: 1)
final residual norm values are on similar orders of magnitudes to ensure that every
variable is converged at the end of each step, and 2) the Jacobian is well-conditioned.</p>

<p>Users may manually set the scaling factor for each variable using the <code class="language-plaintext highlighter-rouge">scaling</code> parameter
when defining the variable. Alternatively, we recommend using the <code class="language-plaintext highlighter-rouge">automatic_scaling</code>
feature from MOOSE. This feature automatically calculates the appropriate scaling factor
for each variable.</p>

<p>We recommend using the following parameters to set automatic scaling in the <code class="language-plaintext highlighter-rouge">Executioner</code> block.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>automatic_scaling = true
resid_vs_jac_scaling_param = 0.2
scaling_group_variables = 'group1 group2; pre1 pre2 pre3 pre4 pre5 pre6 pre7 pre8; temp'
compute_scaling_once = false
</code></pre></div></div>

<ul>
  <li><code class="language-plaintext highlighter-rouge">automatic_scaling</code>: Turns on automatic scaling.</li>
  <li><code class="language-plaintext highlighter-rouge">resid_vs_jac_scaling_param</code>: Determines whether the scaling factor is based on
the residual or Jacobian entries; <code class="language-plaintext highlighter-rouge">0</code> corresponds to pure Jacobian scaling and <code class="language-plaintext highlighter-rouge">1</code> corresponds
to pure residual scaling while all values in between correspond to hybrid scaling. We recommend
setting this parameter within 0-0.3. Going higher than 0.3 risks failure to converge on the
first timestep/<code class="language-plaintext highlighter-rouge">InversePowerMethod</code> iteration unless your initial conditions are very close
to the solution.</li>
  <li><code class="language-plaintext highlighter-rouge">scaling_group_variables</code>: Groups variables which share the same scaling factor.
The MOOSE team recommends grouping variables derived from the same physics for stability.</li>
  <li><code class="language-plaintext highlighter-rouge">compute_scaling_once</code>: Whether Moltres calculates the scaling factors once at the beginning
of the simulation (<code class="language-plaintext highlighter-rouge">true</code>) or at the beginning of every timestep (<code class="language-plaintext highlighter-rouge">true</code>).</li>
</ul>

<h2 id="time-integration-scheme">Time Integration Scheme</h2>

<p>We recommend using either <code class="language-plaintext highlighter-rouge">ImplicitEuler</code> or <code class="language-plaintext highlighter-rouge">BDF2</code> based on the first and second order backward
differentiation formula time integration schemes. The <code class="language-plaintext highlighter-rouge">BDF2</code> scheme is more accurate
(higher-order) and has a
<a href="https://mooseframework.inl.gov/workshop/#/33/4">superior convergence rate</a>.</p>

<p>All multi-stage time integration schemes from MOOSE are incompatible with Moltres simulations.</p>

<p>To set the time integration scheme, include the following line in the <code class="language-plaintext highlighter-rouge">Executioner</code> block.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>scheme = bdf2    # or implicit-euler for ImplicitEuler
</code></pre></div></div>

<h2 id="preconditioning">Preconditioning</h2>

<p>Our discussions on preconditioning here relate to the <code class="language-plaintext highlighter-rouge">NEWTON</code> solve type which relies on PETSc
routines to solve the system of linear equations in each Newton iteration.</p>

<p>Users can pick their preferred linear system solver and the associated settings in PETSc for their
problem through the <code class="language-plaintext highlighter-rouge">petsc_options_iname</code> and <code class="language-plaintext highlighter-rouge">petsc_options_value</code> parameters in the
<code class="language-plaintext highlighter-rouge">Executioner</code> block.</p>

<h4 id="lu">LU</h4>

<p>The most reliable “preconditioner” type for Moltres simulations is <code class="language-plaintext highlighter-rouge">lu</code>. <code class="language-plaintext highlighter-rouge">lu</code> is actually a direct
solver based on LU
factorization. As a direct solver, it is very accurate as long as the user provides the correct
Jacobian formulations in their kernels. PETSc’s default <code class="language-plaintext highlighter-rouge">lu</code> implementation is serial and thus,
it does not scale over multiple processors. <code class="language-plaintext highlighter-rouge">lu</code> requires the <code class="language-plaintext highlighter-rouge">superlu_dist</code> parallel library to
be effective over multiple processors. However, <code class="language-plaintext highlighter-rouge">superlu_dist</code> does not scale as well as the <code class="language-plaintext highlighter-rouge">asm</code>
option in the next section. As such, we recommend only using <code class="language-plaintext highlighter-rouge">superlu_dist</code> for smaller problems
(&lt;100k DOFs).</p>

<p><code class="language-plaintext highlighter-rouge">lu</code> on a single process:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>petsc_options_iname = '-pc_type -pc_factor_shift_type'
petsc_options_value = 'lu       NONZERO'
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">lu</code> with <code class="language-plaintext highlighter-rouge">superlu_dist</code> on multiple MPI processes:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>petsc_options_iname = '-pc_type -pc_factor_shift_type -pc_factor_mat_solver_type'
petsc_options_value = 'lu       NONZERO               superlu_dist'
</code></pre></div></div>

<h4 id="asm">ASM</h4>

<p>Direct solvers like <code class="language-plaintext highlighter-rouge">lu</code> do not scale well with problem size. Iterative methods are recommended
for large problems. The best performing preconditioner type for large problems in Moltres is
<code class="language-plaintext highlighter-rouge">asm</code>. <code class="language-plaintext highlighter-rouge">asm</code> is based on the Additive Schwarz Method (ASM) for generating preconditioners for the
GMRES iterative method. Moltres simulations also require a strong subsolver like <code class="language-plaintext highlighter-rouge">lu</code> for solving
each subdomain.</p>

<p><code class="language-plaintext highlighter-rouge">asm</code> on multiple MPI processes:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>petsc_options_iname = '-pc_type -sub_pc_type -sub_pc_factor_shift_type -pc_asm_overlap -ksp_gmres_restart'
petsc_options_value = 'asm      lu           NONZERO                   1               200'
</code></pre></div></div>

<h4 id="preconditioner-recommendations">Preconditioner Recommendations</h4>

<p>Here are the recommended preconditioner settings for the following problem sizes:</p>

<p>Small problems (&lt;10k DOFs)</p>
<ul>
  <li>Number of MPI processes: 1-4</li>
  <li>Preconditioner options:
    <ul>
      <li><code class="language-plaintext highlighter-rouge">lu</code> on 1 process</li>
      <li><code class="language-plaintext highlighter-rouge">lu</code> with <code class="language-plaintext highlighter-rouge">superlu_dist</code> on multiple processes</li>
      <li><code class="language-plaintext highlighter-rouge">asm</code> on multiple processes</li>
    </ul>
  </li>
</ul>

<p>Medium problems (&lt;100k DOFs)</p>
<ul>
  <li>Number of MPI processes: 4-16</li>
  <li>Preconditioner options:
    <ul>
      <li><code class="language-plaintext highlighter-rouge">lu</code> with <code class="language-plaintext highlighter-rouge">superlu_dist</code> on multiple processes</li>
      <li><code class="language-plaintext highlighter-rouge">asm</code> on multiple processes</li>
    </ul>
  </li>
</ul>

<p>Large problems (&gt;100k DOFs)</p>
<ul>
  <li>Number of MPI processes: Up to 1 MPI process per 10k DOFs</li>
  <li>Preconditioner options:
    <ul>
      <li><code class="language-plaintext highlighter-rouge">asm</code> on multiple processes</li>
    </ul>
  </li>
</ul>

<h2 id="general-tips">General tips</h2>
<ul>
  <li><code class="language-plaintext highlighter-rouge">lu</code> is faster than <code class="language-plaintext highlighter-rouge">asm</code> for small problems.</li>
  <li><code class="language-plaintext highlighter-rouge">l_tol = 1e-5</code> is the default linear tolerance value. <code class="language-plaintext highlighter-rouge">l_tol</code> can be raised to 1e-4 or 1e-3 for
large problems without much impact on performance if the problem requires too many linear
iterations (&gt;400) on every nonlinear iteration.</li>
  <li>Automatic scaling may not scale correctly on the first timestep when restarting a Moltres
simulation using MOOSE’s <code class="language-plaintext highlighter-rouge">Restart</code> functionality. Thus, the simulation may fail to converge on the
first timestep.</li>
  <li>When using <code class="language-plaintext highlighter-rouge">asm</code>, the number of linear iterations required per nonlinear iteration scales
linearly with the number of subdomains (i.e. no. of MPI processes). As such, increasing the number
of processors beyond a certain threshold increases the solve time.</li>
</ul>
:ET